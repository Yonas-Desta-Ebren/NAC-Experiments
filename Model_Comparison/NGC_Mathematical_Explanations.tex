\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{geometry}

\geometry{margin=1in}

\title{Mathematical Explanations of Neural Generative Coding (NGC) Concepts}
\author{}
\date{}

\begin{document}

\maketitle

\section{Generative Neural Coding Network (GNCN) Mathematical Framework}

\subsection{Probabilistic Generative Model}

The NGC framework is based on a hierarchical probabilistic generative model. For a network with $L+1$ layers of neurons (state variables) $N_0, N_1, \ldots, N_L$:

\begin{itemize}
  \item The joint probability distribution is factorized as:
  
  \begin{equation}
    P(z_0=x, z_1, \ldots, z_L) = P(z_0|z_1)P(z_1|z_2)\ldots P(z_{L-1}|z_L)P(z_L)
  \end{equation}

  \item Each conditional probability is typically modeled as a Gaussian distribution:
  
  \begin{equation}
    P(z_{\ell-1}|z_\ell) = \mathcal{N}(z_{\ell-1}; g_\ell(W_\ell \cdot \Phi_\ell(z_\ell)), \Sigma_{\ell-1})
  \end{equation}
\end{itemize}

Where:
\begin{itemize}
  \item $z_\ell \in \mathbb{R}^{J_\ell \times 1}$ is the state vector at layer $\ell$
  \item $g_\ell$ is an activation function
  \item $\Phi_\ell$ is a nonlinear transformation
  \item $W_\ell$ is the generative weight matrix
  \item $\Sigma_{\ell-1}$ is the covariance matrix
\end{itemize}

\subsection{Generative Process}

The generative process in standard NGC models is:

\begin{equation}
  \hat{z}_{\ell-1} = g_\ell(W_\ell \cdot \Phi_\ell(z_\ell))
\end{equation}

For the PDH variant (partially decomposable hierarchy), this becomes:

\begin{equation}
  \hat{z}_{\ell-1} = g_\ell(W_\ell \cdot \Phi_\ell(z_\ell) + \alpha_m \cdot M_{\ell+1} \cdot \Phi_{\ell+1}(z_{\ell+1}))
\end{equation}

Where:
\begin{itemize}
  \item $\hat{z}_{\ell-1}$ is the prediction of layer $\ell-1$ from layer $\ell$
  \item $\alpha_m$ is a binary coefficient (0 or 1)
  \item $M_{\ell+1}$ is an auxiliary generative matrix
\end{itemize}

\section{Error Computation}

\subsection{Error Neurons}

Error neurons compute the prediction errors between actual and predicted states:

For Gaussian distributions (GNCN-t1/Rao and GNCN-t1-$\Sigma$/Friston):
\begin{equation}
  e_\ell = (\Sigma_\ell)^{-1} \odot (z_{\ell-1} - \hat{z}_{\ell-1})
\end{equation}

For the output layer with Bernoulli distribution:
\begin{equation}
  e_0 = (x \oslash \hat{z}_0 - (1-x) \oslash (1-\hat{z}_0))
\end{equation}

Where:
\begin{itemize}
  \item $\odot$ is element-wise multiplication
  \item $\oslash$ is element-wise division
  \item $\Sigma_\ell$ is the covariance matrix (precision-weighting)
\end{itemize}

\subsection{Precision Matrices}

In models with precision weighting (GNCN-t1-$\Sigma$/Friston, GNCN-t2-L$\Sigma$, GNCN-PDH):

\begin{equation}
  \Sigma_\ell = \text{diag}(\sigma_\ell^2)
\end{equation}

Where $\sigma_\ell^2$ are learnable precision parameters that modulate the influence of prediction errors.

\section{State Update Rules}

\subsection{General State Update Equation}

The state update rule for neurons in layer $\ell$ is:

\begin{equation}
  z_\ell^{t+1} = z_\ell^t + \beta \Delta z_\ell^t
\end{equation}

Where:
\begin{itemize}
  \item $\beta$ is the learning rate
  \item $\Delta z_\ell^t$ is the update direction
\end{itemize}

\subsection{Update Direction Calculation}

For GNCN-t1/Rao and GNCN-t1-$\Sigma$/Friston:

\begin{equation}
  \Delta z_\ell^t = -\gamma z_\ell^t + (W_\ell^T \cdot e_{\ell-1}^t) \odot \Phi_\ell'(z_\ell^t) - e_\ell^t
\end{equation}

For GNCN-t2-L$\Sigma$ and GNCN-PDH (without activation derivatives):

\begin{equation}
  \Delta z_\ell^t = -\gamma z_\ell^t + (E_\ell \cdot e_{\ell-1}^t) - e_\ell^t - V_\ell \cdot \Phi_\ell(z_\ell^t)
\end{equation}

Where:
\begin{itemize}
  \item $\gamma$ is the leak/decay parameter
  \item $W_\ell^T$ is the transpose of the generative weights
  \item $E_\ell$ is the learnable error synaptic matrix
  \item $\Phi_\ell'(z_\ell^t)$ is the derivative of the activation function
  \item $V_\ell$ is the lateral connectivity matrix
\end{itemize}

\section{Lateral Competition}

\subsection{Lateral Connectivity Matrix}

The lateral connectivity matrix $V_\ell$ is constructed as:

\begin{equation}
  V_\ell = \alpha_h(M_\ell) \odot (1-I) - \alpha_e(I)
\end{equation}

Where:
\begin{itemize}
  \item $I$ is the identity matrix
  \item $M_\ell$ is a masking matrix
  \item $\alpha_e$ is the self-excitation strength (typically 0.13)
  \item $\alpha_h$ is the lateral inhibition strength (typically 0.125)
\end{itemize}

\subsection{Mask Matrix Generation}

The mask matrix $M_\ell$ for group competition is generated by:

\begin{enumerate}
  \item Creating $J_\ell/K$ matrices of shape $J_\ell \times K$ of zeros: $\{S_1, S_2, \ldots, S_k, \ldots, S_C\}$ (where $C = J_\ell/K$)
  \item In each matrix $S_k$, inserting ones at coordinates $c = \{1, \ldots, k, \ldots, K\}$ and $r = \{1 + K*(k-1), \ldots, k+K*(k-1), \ldots, K+K*(k-1)\}$
  \item Concatenating the matrices horizontally: $M_\ell = \langle S_1, S_2, \ldots, S_C \rangle$
\end{enumerate}

\section{Synaptic Weight Updates}

\subsection{Generative Weight Updates}

For GNCN-t1/Rao and GNCN-t1-$\Sigma$/Friston:

\begin{equation}
  \Delta W_\ell = \eta \cdot e_{\ell-1} \cdot (\Phi_\ell(z_\ell))^T
\end{equation}

For GNCN-t2-L$\Sigma$ and GNCN-PDH:

\begin{equation}
  \Delta W_\ell = \eta \cdot e_{\ell-1} \cdot (\Phi_\ell(z_\ell))^T
\end{equation}

Where $\eta$ is the learning rate for weight updates.

\subsection{Error Synaptic Updates (Type 2 models)}

For GNCN-t2-L$\Sigma$ and GNCN-PDH:

\begin{equation}
  \Delta E_\ell = \eta \cdot z_\ell \cdot (e_{\ell-1})^T
\end{equation}

\subsection{Precision Parameter Updates}

For models with precision weighting:

\begin{equation}
  \Delta \sigma_\ell^2 = \eta_\sigma \cdot ((e_\ell)^2 - \frac{1}{\sigma_\ell^2})
\end{equation}

Where $\eta_\sigma$ is the learning rate for precision parameters.

\subsection{Weight Normalization}

After each update, weight columns are normalized to have unit norm:

\begin{equation}
  W_\ell[:,j] = \frac{W_\ell[:,j]}{||W_\ell[:,j]||_2}
\end{equation}

\section{Non-Gaussian Distributions in NGC}

\subsection{Student's t-Distribution}

When using Student's t-distribution instead of Gaussian:

\begin{equation}
  P(z_{\ell-1}|z_\ell) = \text{St}(z_{\ell-1}; g_\ell(W_\ell \cdot \Phi_\ell(z_\ell)), \Sigma_{\ell-1}, \nu)
\end{equation}

Where $\nu$ is the degrees of freedom parameter.

The error computation becomes:

\begin{equation}
  e_\ell = \frac{(\nu + d_\ell)}{(\nu + \delta_\ell^2)} \cdot (\Sigma_\ell)^{-1} \odot (z_{\ell-1} - \hat{z}_{\ell-1})
\end{equation}

Where:
\begin{itemize}
  \item $d_\ell$ is the dimensionality of layer $\ell$
  \item $\delta_\ell^2 = (z_{\ell-1} - \hat{z}_{\ell-1})^T \cdot (\Sigma_\ell)^{-1} \cdot (z_{\ell-1} - \hat{z}_{\ell-1})$
\end{itemize}

\subsection{Laplace Distribution}

When using Laplace distribution:

\begin{equation}
  P(z_{\ell-1}|z_\ell) = \text{Laplace}(z_{\ell-1}; g_\ell(W_\ell \cdot \Phi_\ell(z_\ell)), b_\ell)
\end{equation}

Where $b_\ell$ is the scale parameter.

The error computation becomes:

\begin{equation}
  e_\ell = \frac{1}{b_\ell} \cdot \text{sign}(z_{\ell-1} - \hat{z}_{\ell-1})
\end{equation}

\subsection{Mixture of Gaussians}

When using a mixture of Gaussians:

\begin{equation}
  P(z_{\ell-1}|z_\ell) = \sum_{k=1}^K \pi_k \cdot \mathcal{N}(z_{\ell-1}; \mu_k, \Sigma_k)
\end{equation}

Where:
\begin{itemize}
  \item $\pi_k$ are the mixture weights
  \item $\mu_k = g_\ell(W_{\ell,k} \cdot \Phi_\ell(z_\ell))$
  \item $\Sigma_k$ are component-specific covariance matrices
\end{itemize}

The error computation involves responsibility-weighted errors from each component.

\section{Inference Process}

\subsection{Iterative Settling}

The inference process involves iteratively updating the state variables until convergence:

\begin{enumerate}
  \item Initialize $z_0 = x$ (clamp input)
  \item Initialize $z_\ell = 0$ for $\ell > 0$
  \item For $t = 1$ to $T$:
  \begin{itemize}
    \item Compute predictions $\hat{z}_{\ell-1}$ for all layers
    \item Compute errors $e_\ell$ for all layers
    \item Update states $z_\ell$ using the update rules
  \end{itemize}
  \item Return final states $z_\ell$
\end{enumerate}

\subsection{Sampling from the Model}

To generate samples from a trained NGC model:

\begin{enumerate}
  \item Initialize $z_L$ with random noise
  \item Run the iterative settling process for $T$ steps
  \item Read out the values from $z_0$
\end{enumerate}

\section{Comparison with Autoencoder Models}

\subsection{Regularized Autoencoder (RAE)}

The objective function for RAE:

\begin{equation}
  \psi = \sum_j [x[j] \log z_0[j] + (1-x[j]) \log(1-z_0[j])] - \lambda \sum_{W_\ell \in \Theta} ||W_\ell||_F^2
\end{equation}

Where:
\begin{itemize}
  \item $z_0$ is the output of the decoder
  \item $\lambda$ is the regularization strength
  \item $||W_\ell||_F$ is the Frobenius norm of weight matrix $W_\ell$
\end{itemize}

\subsection{Gaussian Variational Autoencoder (GVAE)}

The objective function for GVAE:

\begin{equation}
  \psi = \sum_j [x[j] \log z_0[j] + (1-x[j]) \log(1-z_0[j])] - D_{KL}[q(z|x) || p(z)]
\end{equation}

Where:
\begin{itemize}
  \item $q(z|x) = \mathcal{N}(\mu_z, \sigma_z^2)$ is the encoder distribution
  \item $p(z) = \mathcal{N}(0, 1)$ is the prior distribution
  \item $D_{KL}$ is the Kullback-Leibler divergence
\end{itemize}

\subsection{GAN Autoencoder (GAN-AE)}

The objective function for GAN-AE:

\begin{equation}
  \psi = \sum_j [x[j] \log z_0[j] + (1-x[j]) \log(1-z_0[j])] + [\log(D(z_r)) + (1-\log(D(z_f)))]
\end{equation}

Where:
\begin{itemize}
  \item $D$ is the discriminator network
  \item $z_r$ is a sample from the prior distribution
  \item $z_f$ is a sample from the encoder distribution
\end{itemize}

\section{Computational Complexity}

\subsection{Autoencoder Complexity}

For an autoencoder with $L$ layers:
\begin{itemize}
  \item Forward pass: $\mathcal{O}(L)$ matrix multiplications
  \item Backward pass: $\mathcal{O}(L)$ matrix multiplications
  \item Total per sample: $\mathcal{O}(2L)$ matrix multiplications
\end{itemize}

\subsection{NGC Model Complexity}

For an NGC model with $L$ layers and $T$ inference iterations:
\begin{itemize}
  \item Forward pass (predictions): $\mathcal{O}(L \times T)$ matrix multiplications
  \item Error computation: $\mathcal{O}(L \times T)$ element-wise operations
  \item State updates: $\mathcal{O}(L \times T)$ matrix multiplications
  \item Total per sample: $\mathcal{O}(2L \times T)$ matrix multiplications
\end{itemize}

\section{Feature Analysis Mathematics}

\subsection{Feature Composition}

The output of an NGC model can be expressed as a weighted sum of features:

\begin{equation}
  z_0 = \sum_{j=1}^{J_1} z_1[j] \cdot W_1[:,j]
\end{equation}

Where:
\begin{itemize}
  \item $z_1[j]$ is the activation of neuron $j$ in layer 1
  \item $W_1[:,j]$ is the weight vector connecting neuron $j$ to the output layer
\end{itemize}

\subsection{Hierarchical Feature Control}

The activation of neurons in layer 1 is controlled by higher layers:

\begin{equation}
  z_1 = g_1(W_2 \cdot \Phi_2(z_2) + V_1 \cdot \Phi_1(z_1))
\end{equation}

Where the term $W_2 \cdot \Phi_2(z_2)$ represents the top-down control from layer 2, and $V_1 \cdot \Phi_1(z_1)$ represents lateral interactions within layer 1.

\end{document}
